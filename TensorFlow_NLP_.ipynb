{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow NLP .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSTOY5tR7Ve+H5pldvyktg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahidazad/TensorFlow/blob/main/TensorFlow_NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e24EvkEPioRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5ea8e3-0d00-4479-a1a1-08e4efbcf5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lrzXrS62sLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFCn_5y8sPCN",
        "outputId": "cf6b5513-d59d-432d-c360-9ff6dcee4c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDEiS_8OsnI1",
        "outputId": "ea4ce531-5889-4738-8836-ded840a04188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len of data r different but NN work on same len data so we convt data in same len"
      ],
      "metadata": {
        "id": "1iAhIlnysreu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN) # if words more than 250 trim words\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN) # if words less than 250 add 0s\n"
      ],
      "metadata": {
        "id": "GfO4vCUItN9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model creation\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "LH6ps_YV0qbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asmFcm0V2VVj",
        "outputId": "cab6fa9f-8494-4535-9b55-fb7256f19725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model and compile it\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELRKTTJj27I2",
        "outputId": "5c4b1352-fcab-41c0-b307-7abc102e2576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 15s 13ms/step - loss: 0.4183 - acc: 0.8101 - val_loss: 0.2948 - val_acc: 0.8842\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.2370 - acc: 0.9098 - val_loss: 0.2859 - val_acc: 0.8758\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.1836 - acc: 0.9329 - val_loss: 0.3414 - val_acc: 0.8892\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.1502 - acc: 0.9471 - val_loss: 0.3127 - val_acc: 0.8896\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.1317 - acc: 0.9536 - val_loss: 0.2787 - val_acc: 0.8882\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.1110 - acc: 0.9625 - val_loss: 0.3515 - val_acc: 0.8838\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 8s 14ms/step - loss: 0.0970 - acc: 0.9678 - val_loss: 0.3058 - val_acc: 0.8892\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 0.0850 - acc: 0.9719 - val_loss: 0.3134 - val_acc: 0.8840\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.0777 - acc: 0.9746 - val_loss: 0.3475 - val_acc: 0.8872\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.0695 - acc: 0.9778 - val_loss: 0.4243 - val_acc: 0.8790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHlCFrAq6UfQ",
        "outputId": "c6a98393-e9bd-4f7c-f1ae-962e86766f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.5155 - acc: 0.8544\n",
            "[0.5155249834060669, 0.8543999791145325]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcRKbsszY1cD",
        "outputId": "0a0a2f11-eb4b-4ea7-a776-eb41055b1c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "id": "EMcPU1FvageJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43537db-ba91-499c-a15b-dd1635c75549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])   # +ve review value is more then 0.5\n",
        "                     # -ve review value is less then 0.5\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3EF8iu_cjM_",
        "outputId": "4fd73574-81da-4f2f-ae22-aca903e74c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.81522393]\n",
            "[0.3701043]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ul5_l-2lk2pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load data set"
      ],
      "metadata": {
        "id": "u7LdRF1_k3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEmqwR0zj_NE",
        "outputId": "0034c978-b2bd-4f19-9d10-4c7aefe1d426"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load your own data\n",
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "B5B8rkMqcks-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2G39_zDpWMS",
        "outputId": "4e1a261b-53b8-481d-dd38-e13d59804d0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W9oVPaW5rjJ_",
        "outputId": "664605af-713a-4722-8600-2d2d49b2d50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode each character as integer\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n"
      ],
      "metadata": {
        "id": "zZnXP9NirutJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISH_0PkQuQZ2",
        "outputId": "50de9b34-1699-4b3b-fa64-4628eaee3209"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convt numeric to text \n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq5_L_HouyaF",
        "outputId": "7baf0752-2a1a-4011-82a5-7e920b6b1442"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples \n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "YG-cCKeZvtrP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "cGxRY-Jjw2RK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8gg5NeBYxxEF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa1Yhj5szm79",
        "outputId": "c59e781b-1e2c-437e-a101-c572fffee51e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "iQNRRcwkz6ut"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBXbIrFXz7_j",
        "outputId": "4700be56-eb4b-42a3-feb5-5ef0d2b5d901"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data \n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZUJIpzf11y9",
        "outputId": "25c2c596-a5f1-4e71-cef9-1072325723ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyPHveCmGHeL",
        "outputId": "9279b372-27d7-4d5a-ae0c-b21b68eccd56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-2.9517596e-03 -1.3645015e-03  3.0049453e-03 ...  1.5971970e-04\n",
            "    4.6030786e-03 -4.2572394e-03]\n",
            "  [-1.1231718e-03 -1.4847587e-03  4.0665371e-03 ... -9.6902804e-04\n",
            "    2.1882514e-03 -4.3010915e-04]\n",
            "  [ 1.2395799e-04 -2.1531675e-03  4.2029154e-03 ... -2.4640304e-03\n",
            "    4.8488475e-04  2.5326177e-03]\n",
            "  ...\n",
            "  [ 4.8102620e-03 -4.0732515e-03  5.7494442e-04 ...  1.0025585e-02\n",
            "   -1.4560442e-02 -2.4102239e-03]\n",
            "  [-1.1356459e-03 -2.8359997e-03  2.7968120e-03 ...  9.1074994e-03\n",
            "   -7.0728296e-03 -6.6837743e-03]\n",
            "  [ 7.5440328e-03  4.0478846e-03  2.8647778e-03 ...  9.8696072e-03\n",
            "   -1.3563592e-02 -6.0201399e-03]]\n",
            "\n",
            " [[ 3.6676445e-03  1.8619094e-04 -2.8894444e-03 ...  3.5013363e-03\n",
            "   -6.0239132e-04 -2.7726043e-03]\n",
            "  [-1.4306849e-04 -3.3149240e-04 -1.1266513e-03 ...  1.0442856e-03\n",
            "    4.8812712e-04 -4.4053313e-03]\n",
            "  [ 2.5482974e-03  3.7827236e-03 -2.8645676e-03 ... -1.5194742e-03\n",
            "    4.4145723e-04 -4.8154509e-03]\n",
            "  ...\n",
            "  [-7.1699326e-03 -2.2228612e-03 -5.4799668e-03 ... -6.2703863e-03\n",
            "    2.4135089e-03 -4.5890431e-03]\n",
            "  [-1.0186211e-02 -3.3228179e-03 -1.9960711e-03 ... -5.4964614e-03\n",
            "    4.6851048e-03 -6.6549000e-03]\n",
            "  [ 2.6099337e-04  2.4983226e-03 -6.6410424e-04 ... -2.9211384e-03\n",
            "   -5.3340653e-03 -4.4766059e-03]]\n",
            "\n",
            " [[-2.7693966e-03  1.4295332e-03 -3.0935584e-03 ... -4.0373877e-03\n",
            "   -6.2601492e-03 -2.6904405e-03]\n",
            "  [-3.3769286e-03 -2.5011582e-04  4.4479035e-05 ... -2.5548490e-03\n",
            "   -2.7626297e-03 -7.4953763e-03]\n",
            "  [-2.6851974e-03  7.7622099e-04 -4.3068836e-03 ...  1.0046030e-03\n",
            "   -6.1866138e-03 -7.2957380e-03]\n",
            "  ...\n",
            "  [-1.3451772e-03 -7.1488032e-03  5.6572515e-04 ...  1.0632502e-02\n",
            "    2.7297842e-03 -1.2065246e-02]\n",
            "  [-3.8043596e-05 -6.0926029e-03 -2.1507707e-04 ...  1.5566839e-02\n",
            "    8.9858770e-03 -6.8846708e-03]\n",
            "  [ 2.1880341e-03 -1.8381730e-03  3.6082114e-03 ...  1.3647679e-02\n",
            "    8.0003235e-03 -1.0886948e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.0526712e-04  7.1298360e-04 -3.4416858e-03 ...  2.7933279e-03\n",
            "   -3.7493105e-03 -1.6929309e-03]\n",
            "  [ 3.0537138e-03 -2.1728508e-03 -6.4025074e-03 ...  6.0884231e-03\n",
            "   -2.4898765e-03 -3.8727862e-03]\n",
            "  [ 6.8756454e-03  5.8863155e-04 -4.7835670e-03 ...  3.0810023e-03\n",
            "    4.6362705e-04 -1.8611960e-03]\n",
            "  ...\n",
            "  [ 4.7986973e-03 -4.4238439e-04 -1.7275278e-03 ...  2.3784058e-02\n",
            "   -6.9766683e-03 -8.6386083e-03]\n",
            "  [ 3.5785912e-03 -1.9087065e-03 -3.9581237e-03 ...  2.4239330e-02\n",
            "    1.0698617e-03 -4.4813529e-03]\n",
            "  [-3.9508264e-04 -1.2916364e-03 -1.6964094e-03 ...  1.7790688e-02\n",
            "    9.6001662e-05 -6.6817058e-03]]\n",
            "\n",
            " [[ 4.7981529e-03  3.1641000e-03 -6.6671781e-03 ... -1.5460654e-03\n",
            "   -1.8528523e-04  1.0326034e-03]\n",
            "  [ 2.8941813e-03  3.1001922e-03 -8.3214063e-03 ...  6.3276995e-04\n",
            "   -3.2764655e-03 -1.3819586e-03]\n",
            "  [-5.1139947e-04 -1.0459290e-03 -6.9687334e-03 ... -1.5507767e-03\n",
            "    4.1846898e-03 -2.8349913e-03]\n",
            "  ...\n",
            "  [-2.3559681e-03 -1.0475062e-03 -7.0967106e-04 ... -1.0916535e-03\n",
            "   -3.8045787e-03  6.4216047e-03]\n",
            "  [-1.7139134e-03 -1.8392382e-03 -1.6596216e-03 ...  4.1235061e-03\n",
            "    3.8667077e-03  6.6782883e-03]\n",
            "  [-6.1794487e-03 -4.3585501e-03  2.4359277e-03 ...  7.2923214e-03\n",
            "    2.6503415e-03  3.5858399e-03]]\n",
            "\n",
            " [[ 3.2493132e-03 -6.3103903e-03 -5.5075856e-04 ...  5.5299634e-03\n",
            "   -3.0242880e-03 -8.8928617e-04]\n",
            "  [ 3.6412380e-03 -9.1356342e-05 -2.4227328e-03 ...  2.1688524e-03\n",
            "   -2.2708168e-03 -2.2767200e-03]\n",
            "  [-5.5836327e-04 -2.9591404e-04  1.4361774e-03 ...  2.2699614e-03\n",
            "    2.7350378e-03 -5.6029237e-03]\n",
            "  ...\n",
            "  [-6.8379994e-03 -6.6275271e-03  4.3126768e-03 ...  7.9741450e-03\n",
            "   -6.6092191e-03 -7.0361188e-05]\n",
            "  [ 3.4788027e-03  2.0034935e-03  3.9270264e-03 ...  7.0604454e-03\n",
            "   -1.3217881e-02  3.0877304e-04]\n",
            "  [ 2.9606496e-03  6.1137707e-04  4.8482204e-03 ...  1.2736907e-02\n",
            "   -1.1782815e-02  1.0153770e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhiYIuLaGqEH",
        "outputId": "fa0a31ab-93fa-4c62-e76a-9ff1fb3e8551"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00295176 -0.0013645   0.00300495 ...  0.00015972  0.00460308\n",
            "  -0.00425724]\n",
            " [-0.00112317 -0.00148476  0.00406654 ... -0.00096903  0.00218825\n",
            "  -0.00043011]\n",
            " [ 0.00012396 -0.00215317  0.00420292 ... -0.00246403  0.00048488\n",
            "   0.00253262]\n",
            " ...\n",
            " [ 0.00481026 -0.00407325  0.00057494 ...  0.01002559 -0.01456044\n",
            "  -0.00241022]\n",
            " [-0.00113565 -0.002836    0.00279681 ...  0.0091075  -0.00707283\n",
            "  -0.00668377]\n",
            " [ 0.00754403  0.00404788  0.00286478 ...  0.00986961 -0.01356359\n",
            "  -0.00602014]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_pred = pred[0] # look at a prediction \n",
        "print(len(time_pred))\n",
        "print(time_pred) # prediction %\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-WbbyyaHLmi",
        "outputId": "ff9f45e2-f399-4d45-e6d7-cd22208b9c2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-0.00295176 -0.0013645   0.00300495  0.00437595 -0.00615492 -0.00459834\n",
            " -0.00536919  0.00737626  0.00311044 -0.0051773   0.00169396  0.01053201\n",
            " -0.00149254 -0.00037544 -0.00626316  0.00407868  0.00345898  0.00192575\n",
            "  0.00076716  0.00140868  0.00459421 -0.00344135 -0.00970758 -0.00272729\n",
            " -0.00128328 -0.00153946 -0.00397846  0.00217295  0.0022055  -0.00108933\n",
            "  0.00029179  0.00717547  0.0030545   0.00392187 -0.00021276  0.00249266\n",
            "  0.00177693  0.00220856  0.00609797 -0.00066501 -0.00272297  0.00782378\n",
            " -0.00029106  0.00055294 -0.00340767  0.00149444 -0.00017547  0.00125392\n",
            " -0.00707309 -0.00225211  0.00012698 -0.00121573  0.00159804 -0.00217163\n",
            " -0.00444165 -0.00148282 -0.00221159  0.00315557 -0.00113268 -0.00518588\n",
            " -0.00156024  0.00431837  0.00015972  0.00460308 -0.00425724], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution \n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars # predicted charcters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mKoyIvQLJHvU",
        "outputId": "a96d6230-9c95-47c5-98a8-b343af2fbb69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ZCW,VTDfmcFs&;ar:JgHoyjVBS\\nHoE,&UTjTXVSnaWcB  pNIpvKYlxwxNyZosbh3!,DfG\\nymoXVyq?kwjFVk;sIo;E'UHvmgNUv\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to create a loss function that can compare that output to the expected\n",
        "#  output and give us some numeric value representing how close the two were\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "LBHQ2WCLKLsf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)  # compile model"
      ],
      "metadata": {
        "id": "D-Zg3JAKnHyn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "gwxvT9Rym91V"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "\n",
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback]) "
      ],
      "metadata": {
        "id": "3MTLl5kVK13H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971b758f-fa49-4ca7-f335-420aeca0d639"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 15s 65ms/step - loss: 2.5779\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 12s 66ms/step - loss: 1.8804\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.6317\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.4994\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.4203\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.3632\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.3186\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.2794\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.2435\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.2086\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.1731\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.1370\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.1009\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.0621\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.0225\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.9828\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.9417\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.9012\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.8619\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.8244\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.7876\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.7537\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.7235\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6936\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6652\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.6417\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.6211\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5991\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.5815\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5644\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5511\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5364\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5263\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5156\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5039\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.4946\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4863\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.4799\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 17s 74ms/step - loss: 0.4715\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4651\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4600\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4555\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4509\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4474\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4437\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4407\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4366\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4351\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.4309\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "MwxfVLNfm9RW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "s4i5hC_cm3y3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " tf.train.latest_checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhehW7Fnvg8p",
        "outputId": "b2ffeed0-5484-4b96-e447-0fbf9d6da620"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.training.checkpoint_management.latest_checkpoint>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "GE5_3KhYploz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8wtBnYGpnO4",
        "outputId": "33c0668c-499b-462a-f397-d44d971414b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeour best exacting rung, and by earth away.\n",
            "\n",
            "LADY ANNE:\n",
            "Why, thou hast slaughter'd, and these detestable rage,\n",
            "Attern of the Capitol; whom then on thither\n",
            "Hath been thus cauth my poor host from twhither.\n",
            "The time 'twixt him to such power.\n",
            "Uncran, away; AD:\n",
            "No, he is in that state may dear her lives shall follow\n",
            "Ere stuff'd me about, never saw I see\n",
            "you and you have made no offend it. Sir, you show'd like it in my life,\n",
            "Have my disturb'd there chance hath Ros, then,\n",
            "And therefore pardon me your native provenders,\n",
            "And take her here or elsewhere to thy humour friends, God say;\n",
            "Every gently rance, and lead the murderers!\n",
            "Disprehends\n",
            "That had not well for Claudio as well forswear.\n",
            "Look, he rells now to keep my page, my lord.\n",
            "\n",
            "YORK:\n",
            "Ay, with my soul, I would be d life and sent for wall;\n",
            "My leave f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ohqmDXFVuYbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}